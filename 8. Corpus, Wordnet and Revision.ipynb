{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi this is Ritveak.', 'How are U?', 'i AM JUST FINE.']\n"
     ]
    }
   ],
   "source": [
    "m = \"Hi this is Ritveak. How are U? i AM JUST FINE.\"\n",
    "print(sent_tokenize(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan', 'Morgan', 'told', 'himself', 'he', 'would', ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories = 'adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan',\n",
       " 'Morgan',\n",
       " 'told',\n",
       " 'himself',\n",
       " 'he',\n",
       " 'would',\n",
       " 'forget',\n",
       " 'Ann',\n",
       " 'Turner',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'well',\n",
       " 'rid',\n",
       " 'of',\n",
       " 'her',\n",
       " '.',\n",
       " 'He',\n",
       " 'certainly',\n",
       " \"didn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories = 'adventure')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.fileids()\n",
    "inaugural.words(fileids='2009-Obama.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room.n.01\n",
      "room\n",
      "an area within a building enclosed by walls and floor and ceiling\n",
      "['the rooms were very small but they had a nice view']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "  \n",
    "# Then, we're going to use the term \"program\" to find synsets like so: \n",
    "syns = wordnet.synsets(\"room\") \n",
    "  \n",
    "# An example of a synset: \n",
    "print(syns[0].name()) \n",
    "  \n",
    "# Just the word: \n",
    "print(syns[0].lemmas()[0].name()) \n",
    "  \n",
    "# Definition of that first synset: \n",
    "print(syns[0].definition()) \n",
    "  \n",
    "# Examples of the word in use in sentences: \n",
    "print(syns[0].examples()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset(\"room.n.01\").lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mobile.n.01'),\n",
       " Synset('mobile.n.02'),\n",
       " Synset('mobile.n.03'),\n",
       " Synset('mobile.s.01'),\n",
       " Synset('mobile.a.02'),\n",
       " Synset('mobile.s.03'),\n",
       " Synset('mobile.s.04'),\n",
       " Synset('fluid.s.05')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"mobile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobile', 'nomadic', 'peregrine', 'roving', 'wandering']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('mobile.s.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, I am finally happy to get placed!!! #placed #blessed\"\n",
    "tokens = TweetTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'finally',\n",
       " 'happy',\n",
       " 'to',\n",
       " 'get',\n",
       " 'placed',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '#placed',\n",
       " '#blessed']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "e = nltk.corpus.cmudict.entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"addidas's\", ['AH0', 'D', 'IY1', 'D', 'AH0', 'S', 'IH0', 'Z']),\n",
       " ('addie', ['AE1', 'D', 'IY0']),\n",
       " ('adding', ['AE1', 'D', 'IH0', 'NG']),\n",
       " ('addington', ['AE1', 'D', 'IH0', 'NG', 'T', 'AH0', 'N']),\n",
       " ('addis', ['AA1', 'D', 'IH0', 'S']),\n",
       " ('addis-ababa', ['AA1', 'D', 'IH0', 'S', 'AH0', 'B', 'AA1', 'B', 'AH0']),\n",
       " ('addis-ababa', ['AA1', 'D', 'IY0', 'S', 'AH0', 'B', 'AA1', 'B', 'AH0']),\n",
       " ('addison', ['AE1', 'D', 'AH0', 'S', 'AH0', 'N']),\n",
       " ('addison', ['AE1', 'D', 'IH0', 'S', 'AH0', 'N']),\n",
       " (\"addison's\", ['AE1', 'D', 'IH0', 'S', 'AH0', 'N', 'Z']),\n",
       " ('addition', ['AH0', 'D', 'IH1', 'SH', 'AH0', 'N']),\n",
       " ('additional', ['AH0', 'D', 'IH1', 'SH', 'AH0', 'N', 'AH0', 'L']),\n",
       " ('additional', ['AH0', 'D', 'IH1', 'SH', 'N', 'AH0', 'L']),\n",
       " ('additionally', ['AH0', 'D', 'IH1', 'SH', 'AH0', 'N', 'AH0', 'L', 'IY0']),\n",
       " ('additionally', ['AH0', 'D', 'IH1', 'SH', 'N', 'AH0', 'L', 'IY0']),\n",
       " ('additions', ['AH0', 'D', 'IH1', 'SH', 'AH0', 'N', 'Z']),\n",
       " ('additive', ['AE1', 'D', 'AH0', 'T', 'IH0', 'V']),\n",
       " ('additive', ['AE1', 'D', 'IH0', 'T', 'IH0', 'V']),\n",
       " ('additives', ['AE1', 'D', 'AH0', 'T', 'IH0', 'V', 'Z']),\n",
       " ('additives', ['AE1', 'D', 'IH0', 'T', 'IH0', 'V', 'Z']),\n",
       " ('addle', ['AE1', 'D', 'AH0', 'L']),\n",
       " ('addled', ['AE1', 'D', 'AH0', 'L', 'D']),\n",
       " ('addleman', ['AE1', 'D', 'AH0', 'L', 'M', 'AH0', 'N']),\n",
       " ('address', ['AE1', 'D', 'R', 'EH2', 'S']),\n",
       " ('address', ['AH0', 'D', 'R', 'EH1', 'S'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[1000:1025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Mr. X, How are you doing?\n",
      "This is Mr. Ritveak, I hope everything is fine.\n",
      "Don't fall asleep every time!\n",
      "Bye for now.\n",
      "Hello\n",
      ",\n",
      "Mr.\n",
      "X\n",
      ",\n",
      "How\n",
      "are\n",
      "you\n",
      "doing\n",
      "?\n",
      "This\n",
      "is\n",
      "Mr.\n",
      "Ritveak\n",
      ",\n",
      "I\n",
      "hope\n",
      "everything\n",
      "is\n",
      "fine\n",
      ".\n",
      "Do\n",
      "n't\n",
      "fall\n",
      "asleep\n",
      "every\n",
      "time\n",
      "!\n",
      "Bye\n",
      "for\n",
      "now\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize \n",
    "\n",
    "example_text = '''Hello, Mr. X, How are you doing? \n",
    "This is Mr. Ritveak, I hope everything is fine.\n",
    "Don't fall asleep every time! Bye for now.'''\n",
    "\n",
    "for i in sent_tokenize(example_text):\n",
    "    print(i)\n",
    "\n",
    "for i in word_tokenize(example_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr.', 'X', ',', 'show', 'stop', 'word', 'example', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Corpus is basically a fie that stores a lot of text.\n",
    "# We are intrested in Stop words as of now, thats why we will only import that.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "eg = \"Hi there Mr. X, we are here to show a stop word example.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#print(stop_words)\n",
    "# uncomment the above line to see the stop words stored in nltk corpus.\n",
    "\n",
    "words = word_tokenize(eg)\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "\n",
    "#filtered_sentence = [w for w in words if not w in stop_words]\n",
    "\n",
    "# above is a one line code for the 3 line code used above it, feel free to un commen it and\n",
    "# play with it!!!\n",
    "\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "runner\n",
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "run\n",
      ",\n",
      "coz\n",
      "run\n",
      "can\n",
      "make\n",
      "you\n",
      "a\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "eg = [\"run\", \"running\",\"runner\"]\n",
    "for w in eg:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "new_eg = \"It is very important to run, coz running can make you a runner\"\n",
    "\n",
    "#now taking a sentence, tokenizing the words and then finding out the stem words.\n",
    "words = word_tokenize(new_eg)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Ritveak', 'NNP'), (',', ','), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('Btech', 'NNP'), ('student', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('learn', 'VB'), ('NLP', 'NNP'), ('.', '.')]\n",
      "[('And', 'CC'), ('that', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('reason', 'NN'), ('why', 'WRB'), ('I', 'PRP'), ('am', 'VBP'), ('coding', 'VBG'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#pos tagging\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "sample = \"Hey! This is Ritveak, Trying to code and learn NLP.\"\n",
    "test = \"My name is Ritveak, I am a Btech student. I want to learn NLP. And that is the reason why I am coding.\"\n",
    "custom_tokenizer = PunktSentenceTokenizer(sample)\n",
    "\n",
    "tokenized = custom_tokenizer.tokenize(test)\n",
    "\n",
    "def process():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged= nltk.pos_tag(words)\n",
    "            print (tagged)\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (str(e))\n",
    "\n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
